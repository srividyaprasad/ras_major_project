import cv2
import mediapipe as mp
import time
import pyttsx3
import speech_recognition as sr

r = sr.Recognizer()

engine = pyttsx3.init('sapi5')
voices = engine.getProperty('voices')
engine.setProperty('voice', voices[1].id)

def speak(audio):
    engine.say(audio)
    engine.runAndWait()

def listen():
    while(1):
        try:
            
            with sr.Microphone() as source:
                r.adjust_for_ambient_noise(source, duration=0.5)
                print('Listening...')
                audio = r.listen(source)
                print('Recognising...')
                MyText = r.recognize_google(audio)
                MyText = MyText.lower()

                print("User said: "+MyText)
                return MyText
                break      
        except sr.RequestError as e:
            print("Could not request results; {0}".format(e))
        except sr.UnknownValueError:
            print("Failed to recognise, please try again")  

ans='who is this' #listen()
def facerec(imgpath):
    import numpy as np
    import face_recognition
    import os
    from datetime import datetime

    path = r'MajorProject\Dataset'
    images = []
    classNames = []
    myList = os.listdir(path)
    print('Dataset:',myList)
    for cl in myList:
        curImg = cv2.imread(f'{path}/{cl}')
        images.append(curImg)
        classNames.append(os.path.splitext(cl)[0])
    #print(classNames)

    def findEncodings(images):
        encodeList = []
        for img in images:
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            encode = face_recognition.face_encodings(img)[0]
            encodeList.append(encode)
        return encodeList

    def voiceOutput(name, locy):
        speak(name)
        speak('recognised')
        speak('to your')
        speak(locy)

    encodeListKnown = findEncodings(images)
    print('Encoding Complete')
    
    img=face_recognition.load_image_file(imgpath)
    imgS = cv2.resize(img,(0,0),None,0.25,0.25)
    imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)

    facesCurFrame = face_recognition.face_locations(imgS)
    encodesCurFrame = face_recognition.face_encodings(imgS,facesCurFrame)

    for encodeFace,faceLoc in zip(encodesCurFrame,facesCurFrame):
        matches = face_recognition.compare_faces(encodeListKnown,encodeFace)
        faceDis = face_recognition.face_distance(encodeListKnown,encodeFace)
        #print(faceDis)
        matchIndex = np.argmin(faceDis)

        if matches[matchIndex]:
            name = classNames[matchIndex].upper()
            #print(name)
            y1,x2,y2,x1 = faceLoc
            #basic location estimation, must improve with depth cam or algorithm
            if (((x1+x2)/2)>(imgS.shape[1]/2)):
                locy='right'
            else:
                locy='left'
            voiceOutput(name, locy)
            
def facedet():
    pTime = 0
    mpFaceDetection = mp.solutions.face_detection
    mpDraw = mp.solutions.drawing_utils
    faceDetection = mpFaceDetection.FaceDetection(0.75)
    vid=cv2.VideoCapture(0)
    while(1):
        ret, img = vid.read()
        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results=faceDetection.process(imgRGB)
        if results.detections:
            #for id, detection in enumerate(results.detections):
                #int(detection.score[0] * 100)
            imgpath=r'MajorProject\Detected\\dynamicdetectedface.jpg'
            cv2.imwrite(imgpath,img)
            facerec(imgpath) #FACE RECOGNITION TO BE ADDED
        else:
            speak('No face detected')
        break
    
if 'who' in ans:
    facedet()
